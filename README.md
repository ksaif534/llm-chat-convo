# Llama Chat - Local AI Assistant

A web-based chat interface for interacting with the Llama language model locally. This application provides a user-friendly way to communicate with Llama while maintaining conversation context and ensuring data privacy through local processing.

## Features

- **Local Processing**: All conversations are processed locally on your machine
- **Conversation Memory**: Maintains context throughout the chat session
- **Real-time Responses**: Fast response times with local model processing
- **User-friendly Interface**: Clean and intuitive chat interface
- **Secure**: No data sent to external servers

## Prerequisites

- PHP >= 8.1
- Laravel Framework
- Composer
- Ollama

## Complete Setup Guide

### 1. Install Ollama

```bash
curl https://ollama.ai/install.sh | sh
```

- Llama.cpp installed locally
- Node.js and NPM

## Installation

1. Clone the repository:
```bash
git clone <your-repository-url>
cd <repository-name>
```
